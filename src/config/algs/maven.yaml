# --- QMIX specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 500000      # 500k for 3s5z_vs_3s6z, 6h_vs_8z, otherwise 50k

runner: "maven"
batch_size_run: 1

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "maven_learner"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64
skip_connections: False
hyper_initialization_nonzeros: 0

agent: "maven_agent"
mac: "maven_mac"
noise_dim: 5    # 3

mi_loss: 1

discrim_size: 64
discrim_layers: 1
mi_intrinsic: False
mi_scaler: 0.1
hard_qs: False

bandit_epsilon: 0.1
bandit_iters: 8
bandit_batch: 64
bandit_buffer: 512
bandit_reward_scaling: 20
bandit_use_state: True
bandit_policy: True

noise_bandit: True
noise_embedding_dim: 32
noise_bandit_lr: 0.1
noise_bandit_epsilon: 0.2
entropy_scaling: 0.001

# For mpe, we set standarise_rewards True
standardise_rewards: False

name: "maven_bandit_1"